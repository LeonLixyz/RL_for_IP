{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TN-sMTvcG_9"
   },
   "source": [
    "## See README.md file for further details about the project and the environment.\n",
    "\n",
    "### State-Action Description\n",
    "\n",
    "### State\n",
    "State s is an array with give components\n",
    "\n",
    "* s[0]:  constraint matrix $A$of the current LP ($\\max  -c^Tx \\text{ s.t. }Ax \\le  b$) . Dimension is $m \\times n$. See by printing s[0].shape. Here $n$ is the (fixed) number of variables. For instances of size 60 by 60 used in the above command, $n$ will remain fixed as 60. And $m$ is the current number of constraints. Initially, $m$ is to the number of constraints in the IP instance. (For instances generated with --num-c=60, $m$ is 60 at the first step).  But $m$ will increase by one in every step of the episode as one new constraint (cut) is added on taking an action.\n",
    "* s[1]: rhs $b$ for the current LP ($Ax\\le b$). Dimension same as the number $m$ in matrix A.\n",
    "* s[2]: coefficient vector $c$ from the LP objective ($-c^Tx$). Dimension same as the number of variables, i.e., $n$.\n",
    "* s[3],  s[4]: Gomory cuts available in the current round of Gomory's cutting plane algorithm. Each cut $i$ is of the form $D_i x\\le d_i$.   s[3] gives the matrix $D$ (of dimension $k \\times n$) of cuts and s[4] gives the rhs $d$ (of dimension $k$). The number of cuts $k$ available in each round changes, you can find it out by printing the size of last component of state, i.e., s[4].size or s[-1].size.\n",
    "\n",
    "### Actions\n",
    "There are k=s[4].size actions available in each state $s$, with $i^{th}$ action corresponding to the $i^{th}$ cut with inequality $D_i x\\le d_i$ in $s[3], s[4]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSXTKB2zurrt",
    "outputId": "0a8ed989-5fe1-48a5-f22e-1383ee8b7524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.gurobi.com\r\n",
      "Requirement already satisfied: gurobipy in /Users/leon66/miniforge3/lib/python3.10/site-packages (10.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.gurobi.com gurobipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YULy9ymNvDxN",
    "outputId": "dc0eb470-f94d-46a2-d25f-ed6bfb08b621"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import policy_network as PN\n",
    "import helper as H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleonli66\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymenv_v2\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir instances/train_10_n60_m60 idx 0\n",
      "0 :  0.8867413289885917 3.6272668851686043\n",
      "1 :  0.8867413289876822 3.6096191418001737\n",
      "2 :  0.8867413289876822 3.5833160888870066\n",
      "3 :  0.8867413289876822 3.5255436908672793\n",
      "4 :  0.8867413289876822 3.3922460090016315\n",
      "5 :  0.8867413289876822 3.0778622636797093\n",
      "6 :  0.8867413289876822 2.3560433394804288\n",
      "7 :  0.8867413289876822 0.9867708687056597\n",
      "8 :  0.8867413289876822 0.07066179840363455\n",
      "9 :  0.8867413289876822 0.028855769218037758\n",
      "10 :  0.8867413289876822 0.017719559609595384\n",
      "11 :  0.8867413289876822 0.012389322038243373\n",
      "12 :  0.8867413289876822 0.009285832973272123\n",
      "13 :  0.8867413289876822 0.007276146883599551\n",
      "14 :  0.8867413289876822 0.0058821269498526155\n",
      "15 :  0.8867413289876822 0.004867384410170958\n",
      "16 :  0.8867413289876822 0.004101366931505351\n",
      "17 :  0.8867413289876822 0.0035063805577455363\n",
      "18 :  0.8867413289876822 0.0030338183994709877\n",
      "19 :  0.8867413289876822 0.0026513060541010595\n",
      "20 :  0.8867413289876822 0.002337158606753331\n",
      "21 :  0.8867413289876822 0.0020750390296118294\n",
      "22 :  0.8867413289876822 0.0018544142971544874\n",
      "23 :  0.8867413289876822 0.0016668337047274061\n",
      "24 :  0.8867413289876822 0.0015054995288361194\n"
     ]
    }
   ],
   "source": [
    "one_config = {\n",
    "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(1)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create env\n",
    "    \n",
    "    var_size = 61\n",
    "    attention_size = 32\n",
    "    k = 16\n",
    "    hidden_size= 64\n",
    "    lr = 3e-4\n",
    "    Policy = PN.Policy_Network(var_size = var_size, attention_size = attention_size, k = k, hidden_size = hidden_size, lr = lr)\n",
    "    env = make_multiple_env(**one_config)\n",
    "    sigma = 5\n",
    "    gamma = 0.99\n",
    "    # To record traectories generated from current policy\n",
    "    \n",
    "    for e in range(25): \n",
    "\n",
    "        CONSTRAINTS = []  \n",
    "        CANDIDATES = []\n",
    "        ACTS = []\n",
    "        PROBABILITY = []  \n",
    "        REWARDS = [] \n",
    "        total_loss = 0\n",
    "\n",
    "        s = env.reset()   # samples a random instance every time env.reset() is called\n",
    "        d = False\n",
    "        t = 0\n",
    "        repisode = 0\n",
    "\n",
    "        while not d:\n",
    "            #Take a random action\n",
    "            A, b, c0, cuts_a, cuts_b = s\n",
    "            # find attention score\n",
    "            a_b = np.concatenate((A,np.expand_dims(b,-1)),1)\n",
    "            d_e = np.concatenate((cuts_a,np.expand_dims(cuts_b,-1)),1)\n",
    "            total = np.concatenate((a_b, d_e),0)\n",
    "            \n",
    "            total = (total - np.mean(total)) / np.std(total)\n",
    "            #total / np.linalg.norm(total)\n",
    "            \n",
    "            constraint = total[:len(a_b)]\n",
    "            candidate = total[len(a_b):]\n",
    "\n",
    "            CONSTRAINTS.append(constraint)\n",
    "            CANDIDATES.append(candidate)\n",
    "            attention_score = Policy.compute_attention(constraint, candidate)\n",
    "            prob = Policy.compute_prob(attention_score)\n",
    "            \n",
    "            a = np.array([np.argmax(prob)])\n",
    "            ACTS.append(a)\n",
    "\n",
    "            s, r, d, _ = env.step(a)\n",
    "            #print('episode', e, 'step', t, 'reward', r)            \n",
    "            REWARDS.append(r)\n",
    "\n",
    "            t += 1\n",
    "            repisode += r\n",
    "            \n",
    "        \n",
    "        #Below is for logging training performance\n",
    "        rrecord.append(np.sum(REWARDS))\n",
    "        \n",
    "        # TODO:  Use discounted_rewards function to compute \\hat{V}s/\\hat{Q}s  from instant rewards in rews\n",
    "        discounted_r = H.discounted_rewards(REWARDS, gamma)\n",
    "        Q_s = H.evolution_strategies(discounted_r, sigma)\n",
    "        \n",
    "        for contraint,candidate,act,q_s in zip(CONSTRAINTS,CANDIDATES,ACTS,Q_s):\n",
    "            loss = Policy.train(contraint,candidate,act,np.array([q_s]))\n",
    "            total_loss += loss\n",
    "            \n",
    "        print(e, \": \", repisode, total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2xI0riE6md5V",
    "outputId": "0e4b479f-372f-46c2-dea6-4d1b2dd3e7c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:19q7bub5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118cbf956f594c378a4e06518f0b613b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training reward</td><td>▅███▅▇▅▅▅▇▅█▅▅█▅▅▁▁▁▁▁▁▁▅▁▁▁▁▁▂▁▅▁▁▁▁▁▂▅</td></tr><tr><td>training reward moving average</td><td>▁▁▁▁▁▁▁▁██▇▇█▇████▇▆▅▄▃▃▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training reward</td><td>0.60725</td></tr><tr><td>training reward moving average</td><td>0.08968</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">valiant-pyramid-5110</strong>: <a href=\"https://wandb.ai/orcs4529/finalproject/runs/19q7bub5\" target=\"_blank\">https://wandb.ai/orcs4529/finalproject/runs/19q7bub5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221217_173133-19q7bub5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:19q7bub5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d532145b734e485e9d1b216d782217fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016751438200784226, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/leon66/Desktop/Machine Learning/Reinforcement Learning/Final Project/al4263_LeonLi_Project/wandb/run-20221217_173503-1gxgr15h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/orcs4529/finalproject/runs/1gxgr15h\" target=\"_blank\">silvery-valley-5112</a></strong> to <a href=\"https://wandb.ai/orcs4529/finalproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 9\n",
      "0 :  0.9216160135867995 1.3803602398846528e-11\n",
      "1 :  0.6072478073228922 2.4138366226215577e-11\n",
      "2 :  0.7058461509368499 1.3432304682352258e-11\n",
      "3 :  0.7082653196380306 3.263610366594053e-11\n",
      "4 :  1.115349081867862 0.31411148370545194\n",
      "5 :  0.7022608542010857 1.192556964027512e-11\n",
      "6 :  0.8867413289876822 1.924783975805301e-11\n",
      "7 :  0.6151751669476653 2.8960288179393795e-11\n",
      "8 :  1.1150639396978477 9.251448025778687e-05\n",
      "9 :  1.1013110904359564 0.001022679387028024\n",
      "10 :  0.7022608542010857 1.1904416364518174e-11\n",
      "11 :  0.7082653196380306 2.748777295826026e-11\n",
      "12 :  0.6072478073228922 1.7537744917760575e-11\n",
      "13 :  1.1013110904359564 0.0009617268274458877\n",
      "14 :  0.7058461509368499 1.069300727591005e-11\n",
      "15 :  1.115349081867862 0.11208380941023675\n",
      "16 :  0.8867413289876822 1.7775564670321074e-11\n",
      "17 :  0.9216160135867995 5.070363739786756e-12\n",
      "18 :  1.1150639396978477 1.1455716087071997e-05\n",
      "19 :  0.6151751669476653 2.761335382356825e-11\n",
      "20 :  0.7058461509368499 9.20698354408894e-12\n",
      "21 :  0.6072478073228922 1.4905610501755755e-11\n",
      "22 :  0.6151751669476653 2.7613358417874955e-11\n",
      "23 :  0.9216160135867995 5.070259143685036e-12\n",
      "24 :  0.7022608542010857 1.0134704557086571e-11\n",
      "25 :  1.115349081867862 0.049511666623442606\n",
      "26 :  0.7082653196380306 2.4016624803228565e-11\n",
      "27 :  0.8867413289876822 1.7007034010895734e-11\n",
      "28 :  1.115063939699212 2.641557265634321e-06\n",
      "29 :  1.1013110904359564 5.39738908254101e-05\n",
      "30 :  0.7082653196380306 2.401547221980666e-11\n",
      "31 :  0.7058461509368499 8.386163891721068e-12\n",
      "32 :  0.9216160135867995 4.015731743531663e-12\n",
      "33 :  0.6151751669476653 2.7016396388710356e-11\n",
      "34 :  0.6072478073228922 1.3714137568059307e-11\n",
      "35 :  1.115349081867862 0.025472350751338132\n",
      "36 :  0.8867413289876822 1.6513299683950206e-11\n",
      "37 :  0.7022608542010857 8.471369470915273e-12\n",
      "38 :  1.1013110904359564 1.9502820938779928e-05\n",
      "39 :  1.115063939699212 8.805284499674844e-07\n",
      "40 :  0.7058461509368499 7.844305728782819e-12\n",
      "41 :  0.6151751669476653 2.6667674508516117e-11\n",
      "42 :  1.115349081867862 0.014496538673992093\n",
      "43 :  0.7082653196380306 2.2754167819042067e-11\n",
      "44 :  1.115063939699212 3.302074470674067e-07\n",
      "45 :  0.8867413289876822 1.6173312536667446e-11\n",
      "46 :  0.7022608542010857 8.021466469516913e-12\n",
      "47 :  0.9216160135867995 3.138399071128099e-12\n",
      "48 :  0.6072478073228922 1.2676156025142652e-11\n",
      "49 :  1.1013110904359564 8.844255080957139e-06\n"
     ]
    }
   ],
   "source": [
    "run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"training-easy\"])\n",
    "#run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"training-hard\"])\n",
    "#run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"test\"])\n",
    "\n",
    "### TRAINING\n",
    "\n",
    "# Easy Setup: Use the following environment settings. We will evaluate your agent with the same easy config below:\n",
    "easy_config = {\n",
    "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create env\n",
    "    \n",
    "    var_size = 61\n",
    "    attention_size = 32\n",
    "    k = 16\n",
    "    hidden_size= 64\n",
    "    lr = 3e-4\n",
    "    #Policy = PN.Policy_Network(var_size = var_size, attention_size = attention_size, k = k, hidden_size = hidden_size, lr = lr)\n",
    "    env = make_multiple_env(**easy_config)\n",
    "    sigma = 5\n",
    "    gamma = 0.99\n",
    "    rrecord = []\n",
    "\n",
    "    # To record traectories generated from current policy\n",
    "    \n",
    "    for e in range(50): \n",
    "\n",
    "        CONSTRAINTS = []  \n",
    "        CANDIDATES = []\n",
    "        ACTS = []\n",
    "        PROBABILITY = []  \n",
    "        REWARDS = []  \n",
    "\n",
    "        s = env.reset()   # samples a random instance every time env.reset() is called\n",
    "        d = False\n",
    "        t = 0\n",
    "        repisode = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        while not d:\n",
    "            #Take a random action\n",
    "            A, b, c0, cuts_a, cuts_b = s\n",
    "            # find attention score\n",
    "            a_b = np.concatenate((A,np.expand_dims(b,-1)),1)\n",
    "            d_e = np.concatenate((cuts_a,np.expand_dims(cuts_b,-1)),1)\n",
    "            total = np.concatenate((a_b, d_e),0)\n",
    "            \n",
    "            total = (total - np.mean(total)) / np.std(total)\n",
    "            #total / np.linalg.norm(total)\n",
    "            \n",
    "            constraint = total[:len(a_b)]\n",
    "            candidate = total[len(a_b):]\n",
    "\n",
    "            CONSTRAINTS.append(constraint)\n",
    "            CANDIDATES.append(candidate)\n",
    "            attention_score = Policy.compute_attention(constraint, candidate)\n",
    "            prob = Policy.compute_prob(attention_score)\n",
    "            \n",
    "            a = np.array([np.argmax(prob)])\n",
    "            ACTS.append(a)\n",
    "\n",
    "            s, r, d, _ = env.step(a)\n",
    "            #print('episode', e, 'step', t, 'reward', r)            \n",
    "            REWARDS.append(r)\n",
    "\n",
    "            t += 1\n",
    "            repisode += r\n",
    "            \n",
    "        \n",
    "        #Below is for logging training performance\n",
    "        rrecord.append(np.sum(REWARDS))\n",
    "        \n",
    "        # TODO:  Use discounted_rewards function to compute \\hat{V}s/\\hat{Q}s  from instant rewards in rews\n",
    "        discounted_r = H.discounted_rewards(REWARDS, gamma)\n",
    "        Q_s = H.evolution_strategies(discounted_r, sigma)\n",
    "        \n",
    "        for contraint,candidate,act,q_s in zip(CONSTRAINTS,CANDIDATES,ACTS,Q_s):\n",
    "            loss = Policy.train(contraint,candidate,act,np.array([q_s]))\n",
    "            total_loss += loss\n",
    "        print(e, \": \", repisode, total_loss)\n",
    "\n",
    "        fixedWindow=10\n",
    "        movingAverage=0\n",
    "        if len(rrecord) >= fixedWindow:\n",
    "            movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
    "\n",
    "        #wandb logging\n",
    "        wandb.log({ \"training reward\" : rrecord[-1], \"training reward moving average\" : movingAverage})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "PATH = cwd + '/Policy/easy_model'\n",
    "torch.save(Policy, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currect curriculum:  1\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 0\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 1\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 2\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 3\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 4\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 5\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 6\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 7\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 8\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 9\n",
      "0 :  0.6072478073228922 2.4941501629267506\n",
      "1 :  0.7022608541983573 2.8626708992901575\n",
      "2 :  0.6151751669476653 2.4876391899464974\n",
      "3 :  0.7082653196353021 2.8016526706404523\n",
      "4 :  0.9216160135867995 3.54339528182613\n",
      "5 :  1.115349081867862 24.645779786632524\n",
      "6 :  0.8867413289876822 3.3233959258353956e-11\n",
      "7 :  1.1013110904359564 0.43081368743287085\n",
      "8 :  0.7058461509368499 1.8961526513032532e-11\n",
      "9 :  1.1150639396978477 0.46922834640165584\n",
      "10 :  0.7082653196353021 3.165512752531424e-11\n",
      "11 :  0.7058461509368499 1.5900061567614665e-11\n",
      "12 :  1.1013110904359564 0.2736549750867936\n",
      "13 :  0.8867413289876822 2.4928515853744213e-11\n",
      "14 :  0.6072478073228922 2.9522510776725936e-11\n",
      "15 :  0.7022608542010857 1.892335736300916e-11\n",
      "16 :  0.6151751669476653 4.3752273696717884e-11\n",
      "17 :  0.9216160135867995 3.15545972824103e-11\n",
      "18 :  1.1150639396978477 0.2324827517501388\n",
      "19 :  1.115349081867862 0.7935458812613512\n",
      "20 :  0.7082653196353021 2.4105423488652277e-11\n",
      "21 :  1.1013110904359564 0.0878295289885673\n",
      "22 :  0.9216160135867995 1.85467827301114e-11\n",
      "23 :  0.8867413289876822 1.7946972601888134e-11\n",
      "24 :  0.6151751669476653 2.99484912052933e-11\n",
      "25 :  1.115349081867862 0.5218770601246728\n",
      "26 :  0.6072478073228922 1.7794244655779114e-11\n",
      "27 :  0.7022608542010857 1.154250250673524e-11\n",
      "28 :  0.7058461509368499 8.380420206895666e-12\n",
      "29 :  1.1150639396978477 0.011659165403217434\n",
      "currect curriculum:  2\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 0\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 1\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 2\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 3\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 4\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 5\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 6\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 7\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 8\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 9\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 10\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 11\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 12\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 13\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 14\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 15\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 16\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 17\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 18\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 19\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 20\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 21\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 22\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 23\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 24\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 25\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 26\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 27\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 28\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 29\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 30\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 31\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 32\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 33\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 34\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 35\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 36\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 37\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 38\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 39\n",
      "0 :  0.37677874381779475 1.55585087988629e-11\n",
      "1 :  0.7201404175566495 2.0119162384695516e-11\n",
      "2 :  1.115349081867862 0.3566397230927373\n",
      "3 :  1.1150639396978477 0.002691775288268998\n",
      "4 :  1.4756238023867354 1.066576596524793\n",
      "5 :  0.6248641672154918 5.962432681208263e-12\n",
      "6 :  1.3055174419891955 0.6277669365803392\n",
      "7 :  0.7058461509368499 5.936739757475956e-12\n",
      "8 :  1.097205785915321 1.92769051441728e-11\n",
      "9 :  0.8683939133684362 2.11439161545146e-11\n",
      "10 :  0.6151751669476653 2.0583518201173036e-11\n",
      "11 :  0.7022608542010857 7.997224849334498e-12\n",
      "12 :  0.8194501217340076 3.2173656061523776e-11\n",
      "13 :  0.46490663736813076 2.4776852276551962e-11\n",
      "14 :  1.0293603778291072 3.9936212786484085e-06\n",
      "15 :  0.8301311424233973 3.094187329856292e-11\n",
      "16 :  1.7670827665897377 0.8837532787891409\n",
      "17 :  0.6677158110928758 2.0588860222274305e-11\n",
      "18 :  0.6072478073228922 1.075917459626226e-11\n",
      "19 :  1.2037116561025414 0.7609715238315854\n",
      "20 :  0.7082653196380306 1.6866845715956327e-11\n",
      "21 :  1.0726768699150853 4.653589257291711e-06\n",
      "22 :  1.2670405690760163 1.0255233138171747\n",
      "23 :  1.202162290803244 0.1770040501946922\n",
      "24 :  0.7591226471954542 2.559043251671458e-11\n",
      "25 :  0.4771332874629479 1.9586192178790416e-11\n",
      "26 :  0.27260375314835983 7.696892144470117e-11\n",
      "27 :  1.140954722496872 0.6953352437308087\n",
      "28 :  0.43154713802073275 4.6795687406712074e-12\n",
      "29 :  1.1619481483326126 0.021303620220646174\n",
      "currect curriculum:  3\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 0\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 1\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 2\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 3\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 4\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 5\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 6\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 7\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 8\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 9\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 10\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 11\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 12\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 13\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 14\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 15\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 16\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 17\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 18\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 19\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 20\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 21\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 22\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 23\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 24\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 25\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 26\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 27\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 28\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 29\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 30\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 31\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 32\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 33\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 34\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 35\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 36\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 37\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 38\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 39\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 40\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 41\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 42\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 43\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 44\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 45\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 46\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 47\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 48\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 49\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 50\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 51\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 52\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 53\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 54\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 55\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 56\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 57\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 58\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 59\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 60\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 61\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 62\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 63\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 64\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 65\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 66\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 67\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 68\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  0.7777448278875454 1.9222378753922837e-11\n",
      "1 :  0.7356487929473587 3.584179839697475e-11\n",
      "2 :  1.193922700984558 0.3731177820986345\n",
      "3 :  0.9216160135867995 2.5296037911520374e-12\n",
      "4 :  0.2728014378433272 2.6544219351093895e-11\n",
      "5 :  0.37488180020409345 5.305273261674978e-14\n",
      "6 :  0.8157422834224235 7.938566394581424e-12\n",
      "7 :  1.115349081867862 0.01928077951636756\n",
      "8 :  1.0726768699150853 4.282721874068697e-13\n",
      "9 :  0.6064157747389345 2.6604850262568412e-12\n"
     ]
    }
   ],
   "source": [
    "curriculum_10 = {\n",
    "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "\n",
    "curriculum_40 = {\n",
    "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
    "    \"idx_list\"        : list(range(40)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "\n",
    "curriculum_70 = {\n",
    "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
    "    \"idx_list\"        : list(range(70)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "\n",
    "c = [curriculum_10, curriculum_40,curriculum_70]\n",
    "if __name__ == \"__main__\":\n",
    "    # create env\n",
    "    \n",
    "    var_size = 61\n",
    "    attention_size = 32\n",
    "    k = 16\n",
    "    hidden_size= 64\n",
    "    lr = 3e-4\n",
    "    sigma = 5\n",
    "    gamma = 0.95\n",
    "    \n",
    "    Policy = PN.Policy_Network(var_size = var_size, attention_size = attention_size, k = k, hidden_size = hidden_size, lr = lr)\n",
    "    \n",
    "    for i in range(3):\n",
    "        print('currect curriculum: ',i+1)\n",
    "        env = make_multiple_env(**c[i])\n",
    "\n",
    "        # To record traectories generated from current policy\n",
    "\n",
    "        for e in range(30): \n",
    "\n",
    "            CONSTRAINTS = []  \n",
    "            CANDIDATES = []\n",
    "            ACTS = []\n",
    "            PROBABILITY = []  \n",
    "            REWARDS = []  \n",
    "\n",
    "            s = env.reset()   # samples a random instance every time env.reset() is called\n",
    "            d = False\n",
    "            t = 0\n",
    "            repisode = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            while not d:\n",
    "                #Take a random action\n",
    "                A, b, c0, cuts_a, cuts_b = s\n",
    "                # find attention score\n",
    "                a_b = np.concatenate((A,np.expand_dims(b,-1)),1)\n",
    "                d_e = np.concatenate((cuts_a,np.expand_dims(cuts_b,-1)),1)\n",
    "                total = np.concatenate((a_b, d_e),0)\n",
    "\n",
    "                total = (total - np.mean(total)) / np.std(total)\n",
    "\n",
    "                constraint = total[:len(a_b)]\n",
    "                candidate = total[len(a_b):]\n",
    "\n",
    "                CONSTRAINTS.append(constraint)\n",
    "                CANDIDATES.append(candidate)\n",
    "                attention_score = Policy.compute_attention(constraint, candidate)\n",
    "                prob = Policy.compute_prob(attention_score)\n",
    "\n",
    "                a = np.array([np.argmax(prob)])\n",
    "                ACTS.append(a)\n",
    "\n",
    "                s, r, d, _ = env.step(a)\n",
    "                #print('episode', e, 'step', t, 'reward', r)            \n",
    "                REWARDS.append(r)\n",
    "\n",
    "                t += 1\n",
    "                repisode += r\n",
    "\n",
    "            # TODO:  Use discounted_rewards function to compute \\hat{V}s/\\hat{Q}s  from instant rewards in rews\n",
    "            discounted_r = H.discounted_rewards(REWARDS, gamma)\n",
    "            Q_s = H.evolution_strategies(discounted_r, sigma)\n",
    "\n",
    "            for contraint,candidate,act,q_s in zip(CONSTRAINTS,CANDIDATES,ACTS,Q_s):\n",
    "                loss = Policy.train(contraint,candidate,act,np.array([q_s]))\n",
    "                total_loss += loss\n",
    "            print(e, \": \", repisode, total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "PATH = cwd + '/Policy/curriculum_model'\n",
    "torch.save(Policy, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uovbs9i1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ad7807c97b443fada2fb7a031755bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training reward</td><td>▅▆█▃▃▅▃▃▆▁▅▆▄▄▁▆█▃▆▅▃▆▃▄▃▅▅▅▄▄▃▅▃█▃▄▄▆▆▆</td></tr><tr><td>training reward moving average</td><td>▁▁▁▁▁▁▁▁██▇▇▇▇▇▇▇█▇▇▇▇█▇▆▇▆▆▇▇▇▇████▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training reward</td><td>1.25878</td></tr><tr><td>training reward moving average</td><td>0.92723</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">mild-shadow-5011</strong>: <a href=\"https://wandb.ai/orcs4529/finalproject/runs/uovbs9i1\" target=\"_blank\">https://wandb.ai/orcs4529/finalproject/runs/uovbs9i1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221217_150436-uovbs9i1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uovbs9i1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e37fb0de3314da6a2ff81749f046b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01675102846735778, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/leon66/Desktop/Machine Learning/Reinforcement Learning/Final Project/Project_learn2cut/wandb/run-20221217_154629-c3n8x19a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/orcs4529/finalproject/runs/c3n8x19a\" target=\"_blank\">silvery-frost-5030</a></strong> to <a href=\"https://wandb.ai/orcs4529/finalproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir instances/train_100_n60_m60 idx 0\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 1\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 2\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 3\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 4\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 5\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 6\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 7\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 8\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 9\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 10\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 11\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 12\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 13\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 14\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 15\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 16\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 17\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 18\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 19\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 20\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 21\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 22\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 23\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 24\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 25\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 26\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 27\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 28\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 29\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 30\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 31\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 32\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 33\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 34\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 35\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 36\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 37\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 38\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 39\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 40\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 41\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 42\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 43\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 44\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 45\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 46\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 47\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 48\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 49\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 50\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 51\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 52\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 53\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 54\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 55\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 56\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 57\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 58\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 59\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 60\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 61\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 62\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 63\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 64\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 65\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 66\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 67\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 68\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 69\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 70\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 71\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 72\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 73\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 74\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 75\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 76\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 77\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 78\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 79\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 80\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 81\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 82\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 83\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 84\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 85\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 86\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 87\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 88\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 89\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 90\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 91\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 92\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 93\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 94\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 95\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 96\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 97\n",
      "loading training instances, dir instances/train_100_n60_m60 idx 98\n",
      "0 :  0.7058461509368499\n",
      "1 :  1.2186191433333988\n",
      "2 :  0.6151751669476653\n",
      "3 :  1.1057218683463361\n",
      "4 :  1.202162290803244\n",
      "5 :  0.6049971176132658\n",
      "6 :  0.7898175097679996\n",
      "7 :  1.140954722496872\n",
      "8 :  0.996285857943576\n",
      "9 :  0.5032426562058845\n",
      "10 :  1.3408156161538045\n",
      "11 :  1.0726768699150853\n",
      "12 :  1.3055174419891955\n",
      "13 :  1.1952718922998429\n",
      "14 :  0.6248641672154918\n",
      "15 :  0.9748556047572947\n",
      "16 :  0.37677874381779475\n",
      "17 :  1.340161903196531\n",
      "18 :  0.6537341319417465\n",
      "19 :  1.2543115704886532\n",
      "20 :  0.964995842100052\n",
      "21 :  0.6072478073228922\n",
      "22 :  0.7681320403873997\n",
      "23 :  0.27260375314835983\n",
      "24 :  0.8301311424233973\n",
      "25 :  1.2281970839344467\n",
      "26 :  0.7544004101332575\n",
      "27 :  0.9216160135867995\n",
      "28 :  1.3526268798459569\n",
      "29 :  0.19150886322222505\n",
      "30 :  0.7591226471954542\n",
      "31 :  0.5071256399173762\n",
      "32 :  0.37488180020409345\n",
      "33 :  0.9344695540723933\n",
      "34 :  1.0208511766618358\n",
      "35 :  1.1255677057231424\n",
      "36 :  0.22786578133172952\n",
      "37 :  0.33502289829266374\n",
      "38 :  1.1150639396978477\n",
      "39 :  1.097205785915321\n"
     ]
    }
   ],
   "source": [
    "run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"training-hard\"])\n",
    "\n",
    "### TRAINING\n",
    "\n",
    "# Hard Setup: Use the following environment settings. We will evaluate your agent with the same hard config below:\n",
    "hard_config = {\n",
    "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
    "    \"idx_list\"        : list(range(99)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create env\n",
    "    \n",
    "    lr = 3e-4\n",
    "    PATH = cwd + '/Policy/curriculum_model'\n",
    "    Policy = torch.load(PATH)\n",
    "    env = make_multiple_env(**hard_config)\n",
    "    sigma = 5\n",
    "    gamma = 0.99\n",
    "    rrecord = []\n",
    "\n",
    "    # To record traectories generated from current policy\n",
    "    \n",
    "    for e in range(40): \n",
    "\n",
    "        CONSTRAINTS = []  \n",
    "        CANDIDATES = []\n",
    "        ACTS = []\n",
    "        PROBABILITY = []  \n",
    "        REWARDS = []  \n",
    "\n",
    "        s = env.reset()   # samples a random instance every time env.reset() is called\n",
    "        d = False\n",
    "        t = 0\n",
    "        repisode = 0\n",
    "        total_loss\n",
    "        while not d:\n",
    "            #Take a random action\n",
    "            A, b, c0, cuts_a, cuts_b = s\n",
    "            # find attention score\n",
    "            a_b = np.concatenate((A,np.expand_dims(b,-1)),1)\n",
    "            d_e = np.concatenate((cuts_a,np.expand_dims(cuts_b,-1)),1)\n",
    "            total = np.concatenate((a_b, d_e),0)\n",
    "\n",
    "            total = (total - np.mean(total)) / np.std(total)\n",
    "            #total / np.linalg.norm(total)\n",
    "            \n",
    "            constraint = total[:len(a_b)]\n",
    "            candidate = total[len(a_b):]\n",
    "\n",
    "            CONSTRAINTS.append(constraint)\n",
    "            CANDIDATES.append(candidate)\n",
    "            attention_score = Policy.compute_attention(constraint, candidate)\n",
    "            prob = Policy.compute_prob(attention_score)\n",
    "            \n",
    "            a = np.array([np.argmax(prob)])\n",
    "            ACTS.append(a)\n",
    "\n",
    "            s, r, d, _ = env.step(a)\n",
    "            #print('episode', e, 'step', t, 'reward', r)            \n",
    "            REWARDS.append(r)\n",
    "\n",
    "            t += 1\n",
    "            repisode += r\n",
    "            \n",
    "        \n",
    "        #Below is for logging training performance\n",
    "        rrecord.append(np.sum(REWARDS))\n",
    "        \n",
    "        # TODO:  Use discounted_rewards function to compute \\hat{V}s/\\hat{Q}s  from instant rewards in rews\n",
    "        discounted_r = H.discounted_rewards(REWARDS, gamma)\n",
    "        Q_s = H.evolution_strategies(discounted_r, sigma)\n",
    "        \n",
    "        for contraint,candidate,act,q_s in zip(CONSTRAINTS,CANDIDATES,ACTS,Q_s):\n",
    "            loss = Policy.train(contraint,candidate,act,np.array([q_s]))\n",
    "            total_loss += loss\n",
    "        print(e, \": \", repisode, total_loss)\n",
    "\n",
    "        fixedWindow=10\n",
    "        movingAverage=0\n",
    "        if len(rrecord) >= fixedWindow:\n",
    "            movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
    "\n",
    "        #wandb logging\n",
    "        wandb.log({ \"training reward\" : rrecord[-1], \"training reward moving average\" : movingAverage})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "PATH = cwd + '/Policy/hard_model'\n",
    "torch.save(Policy, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"test\"])\n",
    "\n",
    "### testing\n",
    "\n",
    "custom_config = {\n",
    "    \"load_dir\"        : 'instances/randomip_n60_m60',   # this is the location of the randomly generated instances (you may specify a different directory)\n",
    "    \"idx_list\"        : list(range(20)),                # take the first 20 instances from the directory\n",
    "    \"timelimit\"       : 50,                             # the maximum horizon length is 50\n",
    "    \"reward_type\"     : 'obj'                           # DO NOT CHANGE reward_type\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create env\n",
    "    \n",
    "    PATH = cwd + '/Policy/curriculum_model'\n",
    "    Policy = torch.load(PATH)\n",
    "    env = make_multiple_env(**custom_config)\n",
    "    rrecord = []\n",
    "\n",
    "    # To record traectories generated from current policy\n",
    "    \n",
    "    for e in range(40): \n",
    "        s = env.reset()   # samples a random instance every time env.reset() is called\n",
    "        d = False\n",
    "        t = 0\n",
    "        repisode = 0\n",
    "        while not d:\n",
    "            #Take a random action\n",
    "            A, b, c0, cuts_a, cuts_b = s\n",
    "            # find attention score\n",
    "            a_b = np.concatenate((A,np.expand_dims(b,-1)),1)\n",
    "            d_e = np.concatenate((cuts_a,np.expand_dims(cuts_b,-1)),1)\n",
    "            total = np.concatenate((a_b, d_e),0)\n",
    "\n",
    "            total = (total - np.mean(total)) / np.std(total)\n",
    "            #total / np.linalg.norm(total)\n",
    "            \n",
    "            constraint = total[:len(a_b)]\n",
    "            candidate = total[len(a_b):]\n",
    "\n",
    "            attention_score = Policy.compute_attention(constraint, candidate)\n",
    "            prob = Policy.compute_prob(attention_score)\n",
    "            a = np.array([np.argmax(prob)])\n",
    "\n",
    "            s, r, d, _ = env.step(a)\n",
    "            t += 1\n",
    "            repisode += r\n",
    "            \n",
    "        \n",
    "        #Below is for logging training performance\n",
    "        rrecord.append(repisode)\n",
    "        \n",
    "        print(e, \": \", repisode)\n",
    "\n",
    "        fixedWindow=10\n",
    "        movingAverage=0\n",
    "        if len(rrecord) >= fixedWindow:\n",
    "            movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
    "\n",
    "        #wandb logging\n",
    "        wandb.log({ \"training reward\" : rrecord[-1], \"training reward moving average\" : movingAverage})\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
